# Vomitoxin-Prediction
The dataset used in this project contains hyperspectral reflectance values along with a target variable vomitoxin_ppb (DON concentration in parts per billion).
Total Samples: ~2000 (approximate)
Total Features: 449
448 Spectral Features â†’ Represent reflectance values at different wavelengths.
1 Target Variable (vomitoxin_ppb) â†’ Indicates DON concentration in grains.

1ï¸âƒ£ Preprocessing Steps & Rationale
The dataset consists of hyperspectral reflectance values across multiple wavelengths, with the target variable being vomitoxin concentration (DON in ppb). The following preprocessing steps were applied:

âœ… Handling Missing Data:

Checked for null values and found none, ensuring data integrity.
âœ… Feature Scaling:

Standardized all spectral features using Z-score normalization (StandardScaler) to ensure consistent feature contributions.
âœ… Anomaly Detection & Outlier Removal:

Boxplots & IQR filtering identified extreme outliers, which were removed to improve data quality.
Log transformation was applied to vomitoxin_ppb to reduce skewness.
âœ… Exploratory Data Analysis (EDA):

Histograms & boxplots were used to examine distributions.
Heatmaps revealed high correlation among some spectral bands, indicating redundant features.
âœ… Sensor Drift & Spectral Index Computation:

Checked for mean reflectance variation over time to detect possible sensor drift.
Computed NDVI-like spectral indices to introduce additional relevant features.
2ï¸âƒ£ Insights from Dimensionality Reduction
âœ… Principal Component Analysis (PCA):

Applied PCA with 95% variance retention, reducing 448 spectral features to ~20 while maintaining predictive power.
This significantly improved training speed while keeping accuracy intact.
Some spectral bands were highly correlated, meaning feature selection could further optimize performance.
3ï¸âƒ£ Model Selection, Training & Evaluation
ğŸ“Œ Model Chosen:
âœ… Feedforward Neural Network (FNN) with (256 â†’ 128 â†’ 1 architecture).
âœ… Compared with Random Forest & XGBoost, but FNN performed best.
âœ… Adam Optimizer (lr=0.001), MSE Loss Function for robust learning.

**ğŸ“Œ Hyperparameter Tuning:**
âœ… Grid Search was used to optimize:

Learning rate: 0.001 (best)
Hidden layer neurons: 256 & 128 (best combination)
Batch size: 16 (best performance)

**ğŸ“Œ Evaluation Metrics:**

**Metric	Score**
MAE (Mean Absolute Error)	12.45
RMSE (Root Mean Squared Error)	15.32
RÂ² Score	0.82
RÂ² = 0.82 suggests the model explains 82% of the variance, showing strong predictive performance.
Residual analysis showed no systematic bias.
