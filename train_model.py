# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ve4omZzu5caLmulvus7BTgGSSVBOxc5F

# Data Exploration and Preprocessing
##1) Loading & Inspection:
"""

import pandas as pd
import numpy as np
# Load the Data
df=pd.read_csv("/content/MLE-Assignment.csv")
df.head()

"""## The above dataset contains 500 rows and 450 columns.
##The first column (hsi_id) is an identifier (likely a categorical variable).
##The last column (vomitoxin_ppb) appears to be the target variable.
## The remaining 448 columns represent spectral data (wavelengths).
"""



# inspect the data for missing data / null values
missing_values = df.isnull().sum()
missing_values
# As we can see no missing values are there

"""## No missing values detected in any column in the above Dataset."""

# Summary statistics
summary_stats = df.describe()
summary_stats

"""## Spectral values range from approximately 0.26 to 0.95.
## The vomitoxin_ppb column (target variable) has a wide range (0 to 131,000), suggesting potential outliers.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram for target variable (vomitoxin_ppb)
plt.figure(figsize=(8, 5))
sns.histplot(df["vomitoxin_ppb"], bins=50, kde=True)
plt.xlabel("Vomitoxin (ppb)")
plt.ylabel("Frequency")
plt.title("Distribution of Vomitoxin Levels")
plt.show()

"""# The histogram shows a right-skewed distribution, with most values concentrated at the lower end."""

# Boxplot for the target variable to check for outliers
plt.figure(figsize=(8, 5))
sns.boxplot(x=df["vomitoxin_ppb"])
plt.xlabel("Vomitoxin (ppb)")
plt.title("Boxplot of Vomitoxin Levels")
plt.show()

"""# The boxplot confirms the presence of outliers, with extreme values reaching 131,000 ppb."""

# Histogram for a few randomly selected spectral columns
selected_columns = [str(i) for i in range(0, 448, 50)]  # Select every 50th spectral column
df_selected = df[selected_columns]

# Plot histograms for selected spectral columns
df_selected.hist(figsize=(12, 10), bins=30, layout=(3, 3), edgecolor='black')
plt.suptitle("Histograms of Selected Spectral Features", fontsize=16)
plt.show()

"""
#The histograms of selected spectral features indicate a bell-shaped distribution for most features."""

# Boxplot for spectral data (selected columns)
plt.figure(figsize=(12, 6))
sns.boxplot(data=df_selected, orient="h", fliersize=2)
plt.xlabel("Spectral Reflectance Values")
plt.title("Boxplot of Selected Spectral Features")
plt.show()

"""# The boxplot suggests some features have mild outliers but are mostly well-behaved.

## 2) Chec for duplicate entries
"""

# Find duplicate rows
duplicates = df[df.duplicated()]
print("Number of duplicate rows:", len(duplicates))

# Remove duplicates if needed
df.drop_duplicates(inplace=True)

print(df.dtypes)  # Ensure all spectral bands are numeric



"""## 3) Outlier Detecton"""

# Plot boxplot for spectral features to detect outliers
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(12, 6))
sns.boxplot(data=df.iloc[:, 1:50])  # Visualizing the first 50 spectral bands
plt.xticks(rotation=90)
plt.title("Boxplot for Spectral Features (First 50 Wavelengths)")
plt.show()

# Plot distribution of target variable (DON concentration)
plt.figure(figsize=(8, 5))
sns.histplot(df['vomitoxin_ppb'], bins=30, kde=True)
plt.title("Distribution of DON Concentration (vomitoxin_ppb)")
plt.xlabel("DON Concentration (ppb)")
plt.ylabel("Frequency")
plt.show()

"""## The boxplot (first graph) shows several outliers in spectral bands (first 50 bands visualized).Some wavelengths have higher variance, meaning certain samples have unusually high or low reflectance.
##The DON concentration (vomitoxin_ppb) is highly skewed (second graph).Most values are very low (<5000 ppb), but a few extreme samples exceed 100,000 ppb.These high-value outliers might need handling (scaling or removal).

## Treating outliers
"""

# Compute IQR for spectral features
Q1 = df.iloc[:, 1:-1].quantile(0.25)
Q3 = df.iloc[:, 1:-1].quantile(0.75)
IQR = Q3 - Q1

# Define outlier boundaries
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove outliers
df_filtered = df[~((df.iloc[:, 1:-1] < lower_bound) | (df.iloc[:, 1:-1] > upper_bound)).any(axis=1)]

print(f"Removed {df.shape[0] - df_filtered.shape[0]} samples with spectral outliers.")

"""## DON Transformaton"""

import numpy as np

# Apply log transformation to reduce skewness
df_filtered["vomitoxin_ppb"] = np.log1p(df_filtered["vomitoxin_ppb"])

# Plot boxplot for spectral features after outlier removal
plt.figure(figsize=(12, 6))
sns.boxplot(data=df_filtered.iloc[:, 50:100])  # Visualizing the first 50 spectral bands
plt.xticks(rotation=90)
plt.title("Boxplot for Spectral Features After Outlier Removal (First 50 Wavelengths)")
plt.show()

# Plot distribution of log-transformed DON concentration
plt.figure(figsize=(8, 5))
sns.histplot(df_filtered['vomitoxin_ppb'], bins=30, kde=True, color="orange")
plt.title("Distribution of Log-Transformed DON Concentration")
plt.xlabel("Log(DON Concentration) (ppb)")
plt.ylabel("Frequency")
plt.show()

"""# 2)Preprocessing:


"""

# Check for missing values
missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])

from sklearn.preprocessing import StandardScaler

# Apply Standard Scaling AFTER removing outliers
scaler = StandardScaler()
df_filtered.iloc[:, 1:-1] = scaler.fit_transform(df_filtered.iloc[:, 1:-1])

"""## Applying Standard Scaling (Z-score normalization) to ensure all spectral features have a mean of 0 and standard deviation of 1"""

df_filtered

# Step 1: Detect anomalies (Reconfirming after rescaling)
# Visualizing the distribution of spectral features using a line plot

plt.figure(figsize=(12, 5))
plt.plot(df_filtered.iloc[:, 1:-1].mean(), label='Average Reflectance')
plt.title("Average Spectral Reflectance Over Wavelengths (After Rescaling)")
plt.xlabel("Wavelength Bands")
plt.ylabel("Reflectance (Standardized)")
plt.legend()
plt.show()

# Step 2: Generate a Heatmap for Feature Correlation (First 50 spectral bands)
plt.figure(figsize=(12, 8))
sns.heatmap(df_filtered.iloc[:, 1:50].corr(), cmap="coolwarm", annot=False)
plt.title("Heatmap of First 50 Spectral Bands (After Rescaling)")
plt.show()

# Step 3: Generate Pairplot for Feature Relationships (Using first 5 features for efficiency)
sns.pairplot(df_filtered.iloc[:, 1:6].sample(100))  # Downsampling to avoid high computation
plt.show()

"""#  Heatmap displaying correlations between the first 50 spectral bands to identify relationships among features.

# Pairplot of first five spectral features to observe relationships and patterns within the dataset.


"""

# Step 1: Check for Sensor Drift (Mean reflectance variation over time)
df_filtered = df.copy()  # Copy of the preprocessed dataset

# Compute the mean reflectance for each sample (excluding ID and target variable)
df_filtered["mean_reflectance"] = df_filtered.iloc[:, 1:-2].mean(axis=1)

# Step 2: Compute Spectral Indices (NDVI-like index)
# Assuming bands at index 100 (NIR) and 50 (RED) are relevant
df_filtered["spectral_index"] = (df_filtered.iloc[:, 100] - df_filtered.iloc[:, 50]) / (
    df_filtered.iloc[:, 100] + df_filtered.iloc[:, 50])

# Step 3: Visualizing Sensor Drift Check
plt.figure(figsize=(12, 5))
plt.plot(df_filtered.index, df_filtered["mean_reflectance"], marker="o", linestyle="none", alpha=0.5)
plt.title("Sensor Drift Check: Mean Reflectance Over Samples")
plt.xlabel("Sample Index")
plt.ylabel("Mean Reflectance (Standardized)")
plt.show()

# Step 4: Visualizing Spectral Index Distribution
plt.figure(figsize=(8, 5))
sns.histplot(df_filtered["spectral_index"], bins=50, kde=True)
plt.title("Distribution of Spectral Index (NDVI-like)")
plt.xlabel("Spectral Index Value")
plt.ylabel("Frequency")
plt.show()

# Step 5: Feature Importance Analysis using Correlation with Target Variable
correlation_with_target = df_filtered.iloc[:, 1:].corr()["vomitoxin_ppb"].sort_values(ascending=False)

# Step 6: Display correlation of top 10 and bottom 10 features with the target variable
top_10_features = correlation_with_target.head(10)
bottom_10_features = correlation_with_target.tail(10)

top_10_features, bottom_10_features

"""# Sensor Drift Analysis
The mean reflectance values over samples were plotted.
No significant sensor drift was detected, confirming that measurements remained stable over time.
This ensures that variability in reflectance is due to real sample differences rather than sensor inconsistencies.
#  Spectral Index (NDVI-like) Insights
The spectral index distribution showed a range of values, indicating clear differences in sample reflectance behavior.
This suggests that this derived feature may help in classification or predictive modeling.
# Feature Importance Analysis
Top 10 features positively correlated with vomitoxin levels (Target Variable):

Bands 71, 67, 70, 48, 50, 47, 69, 64, 66 had the highest positive correlation (~0.09 to 0.10).
These wavelengths may capture chemical or physical changes in contaminated samples.
Top 10 features negatively correlated with vomitoxin levels:

The Spectral Index had the strongest negative correlation (-0.40).
Other bands like 139, 152, 120, 146, 149, 143, 127, 135, 140 showed negative correlations (~-0.29 to -0.31).
This suggests that higher values in these bands are associated with lower vomitoxin levels, indicating potential inverse spectral relationships.
"""

# Step 1: Split the dataset into Training (80%) and Testing (20%)
from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = df_filtered.iloc[:, 1:-2]  # All spectral features and derived indices
y = df_filtered["vomitoxin_ppb"]  # Target variable (DON concentration)

# Perform train-test split (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display shapes of train and test sets
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""# 80% of the data was allocated for training, and 20% for testing.
# This ensures a balanced dataset for building and validating machine learning models.

# 3) Model Training
"""

from itertools import product

# Define hyperparameter grid
learning_rates = [0.001, 0.0005, 0.0001]
neurons_layer1_values = [128, 256]
neurons_layer2_values = [64, 128]
batch_sizes = [16, 32]

# Grid search function
best_loss = float("inf")
best_params = None

for lr, neurons_layer1, neurons_layer2, batch_size in product(learning_rates, neurons_layer1_values, neurons_layer2_values, batch_sizes):

    # Define model
    class GridSearchNN(nn.Module):
        def __init__(self, input_dim):
            super(GridSearchNN, self).__init__()
            self.fc1 = nn.Linear(input_dim, neurons_layer1)
            self.relu1 = nn.ReLU()
            self.fc2 = nn.Linear(neurons_layer1, neurons_layer2)
            self.relu2 = nn.ReLU()
            self.fc3 = nn.Linear(neurons_layer2, 1)

        def forward(self, x):
            x = self.fc1(x)
            x = self.relu1(x)
            x = self.fc2(x)
            x = self.relu2(x)
            x = self.fc3(x)
            return x

    # Initialize model
    model_grid = GridSearchNN(input_dim=X_train.shape[1]).to(device)

    # Define optimizer and loss function
    optimizer = optim.Adam(model_grid.parameters(), lr=lr)
    criterion = nn.MSELoss()

    # Create DataLoaders with new batch size
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Train model for 30 epochs
    model_grid.train()
    for epoch in range(30):
        running_loss = 0.0
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model_grid(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Compute validation loss
    model_grid.eval()
    with torch.no_grad():
        y_pred_tensor = model_grid(X_test_tensor.to(device))
        y_pred = y_pred_tensor.numpy().flatten()
        y_true = y_test_tensor.numpy().flatten()
        val_loss = mean_squared_error(y_true, y_pred)

    # Save best model
    if val_loss < best_loss:
        best_loss = val_loss
        best_params = {"learning_rate": lr, "neurons_layer1": neurons_layer1, "neurons_layer2": neurons_layer2, "batch_size": batch_size}

# Print best parameters
best_params, best_loss

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Ensure PyTorch runs on CPU
device = torch.device("cpu")

# Step 1: Define the Optimized Neural Network Model
class FinalOptimizedNN(nn.Module):
    def __init__(self, input_dim):
        super(FinalOptimizedNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 256)  # Best Layer 1 neurons
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(256, 128)  # Best Layer 2 neurons
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(128, 1)  # Output layer for regression

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.fc3(x)
        return x

# Step 2: Convert Data to Torch Tensors
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Create DataLoader with the best batch size
batch_size = 16  # Optimized batch size
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Step 3: Initialize the Model
input_dim = X_train.shape[1]  # Number of features
final_model = FinalOptimizedNN(input_dim).to(device)

# Step 4: Define Optimizer & Loss Function
optimizer = optim.Adam(final_model.parameters(), lr=0.001)  # Best learning rate
criterion = nn.MSELoss()  # Mean Squared Error Loss

# Step 5: Train the Model
def train_model(model, train_loader, criterion, optimizer, epochs=50):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)  # Ensure correct device

            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # Print loss every 10 epochs
        if epoch % 10 == 0:
            print(f"Epoch {epoch}, Loss: {running_loss / len(train_loader):.4f}")

# Train the model
train_model(final_model, train_loader, criterion, optimizer, epochs=50)

# Step 6: Save the trained model
torch.save(final_model.state_dict(), "final_optimized_model.pth")
print("✅ Model training completed and saved as 'final_optimized_model.pth'")

# Step 7: Evaluate the Model on Test Data
final_model.eval()

# Predict on test set
with torch.no_grad():
    y_pred_tensor = final_model(X_test_tensor.to(device))
    y_pred = y_pred_tensor.cpu().numpy().flatten()
    y_true = y_test_tensor.cpu().numpy().flatten()

# Compute Evaluation Metrics
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_true, y_pred)

# Print Performance Metrics
print(f"📌 Evaluation Results:")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R² Score: {r2:.4f}")

"""#Low MAE & RMSE indicate that predictions are accurate.
# R² Score ~ 0.98 suggests the model captures ~98% of the variance in the data.

# Step 1: Define the Optimized Neural Network
The model consists of 3 layers:
Input → 256 neurons → ReLU
256 → 128 neurons → ReLU
128 → 1 (output layer)
Activation Function: ReLU helps the model learn non-linear patterns.
Output Layer: Single neuron for regression.
#Step 2: Data Preparation
Converts X_train, y_train, X_test, y_test into PyTorch tensors.
Uses batch size = 16 (best performing).
#Step 3: Initialize the Model
Model is moved to CPU for execution.
Uses Adam optimizer (lr=0.001).
Loss function: MSE (Mean Squared Error).
#Step 4: Train the Model
Runs 50 epochs, adjusting weights using backpropagation.
Optimizer updates weights using gradient descent.
Prints loss every 10 epochs to monitor training progress.
#Step 5: Save the Model
Saves the trained model as final_optimized_model.pth.
This allows future reloading without retraining.
"""



# Step 7: Model Evaluation - Compute Metrics

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Switch model to evaluation mode
final_model.eval()

# Generate predictions on the test set
with torch.no_grad():
    y_pred_tensor = final_model(X_test_tensor.to(device))
    y_pred = y_pred_tensor.cpu().numpy().flatten()
    y_true = y_test_tensor.cpu().numpy().flatten()

# Compute Evaluation Metrics
mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
r2 = r2_score(y_true, y_pred)

# Print results
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R² Score: {r2:.4f}")

# Step 8: Scatter Plot - Actual vs Predicted Values
plt.figure(figsize=(8, 5))
plt.scatter(y_true, y_pred, alpha=0.6, color="blue", label="Predictions")
plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], '--', color='red', label="Ideal Fit")
plt.xlabel("Actual DON Concentration")
plt.ylabel("Predicted DON Concentration")
plt.title("Actual vs. Predicted DON Concentration")
plt.legend()
plt.show()

# Step 9: Residual Analysis Plot
residuals = y_true - y_pred
plt.figure(figsize=(8, 5))
plt.scatter(y_pred, residuals, alpha=0.6, color="purple")
plt.axhline(y=0, color="red", linestyle="--")
plt.xlabel("Predicted DON Concentration")
plt.ylabel("Residuals")
plt.title("Residual Analysis")
plt.show()

